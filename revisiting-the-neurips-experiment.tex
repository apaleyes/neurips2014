\documentclass[twoside]{article}

\usepackage{aistats2021}
% If your paper is accepted, change the options for the package
% aistats2021 as follows:
%
%\usepackage[accepted]{aistats2021}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}
\usepackage{hyperref}

\usepackage{todonotes}
\usepackage[margin=2.5cm]{geometry}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{fancyvrb}
\usepackage{todo}



\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[
\aistatstitle{Inconsistency in Conference Peer Review: Revisiting the NeurIPS Experiment}
\aistatsauthor{Neil D. Lawrence \And Corinna Cortes}
\aistatsaddress{University of Cambridge \And  Google Research, New York}
]

\begin{abstract}
    In this paper we revisit the NeurIPS experiment that examined
inconsistency in conference peer review. We determine that the
underlying cause of inconsistency in the reviews was subjectivity in the
reviewers' quality scores: after score calibration 50\% of the variation
in reviewer scores was subjective in origin. Further, with seven years
passing since the experiment we find that for \emph{accepted} papers,
there is no correlation between quality scores and impact of the paper
as measured as a function of citation count. We trace the fate of
rejected papers, recovering where these papers were eventually
published. For these papers we find a correlation between quality scores
and impact. We conclude that the reviewing process for the 2014
conference was good for identifying poor papers, but poor for
identifying good papers. We give some suggestions for improving the
reviewing process but also warn against removing the subjective element.
Finally, we suggest that the real conclusion of the experiment is that
the community should place less onus on the notion of `top tier
conference publications' when assessing the quality of individual
researchers.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In 2014 the Program Chairs of the NeurIPS conference, Corinna Cortes
and Neil Lawrence, implemented the NeurIPS experiment. The experiment
was designed to assess the consistency of the conference peer
reviewing process. From the conference 10\% of the papers were
randomly chosen to be reviewed by two independent program
committees. The objective was to determine if decision making was
consistent across these two committees.  The results showed that the
decisions between the two committees were 74\% consistent as compared
with a random committee that would have been 62.5\% consistent. While
the review process can be shown to be significantly better than a
random committee (see Appendix \ref{a-random-committee-25} in the
supplementary material for an analysis), the reaction from some in the
community was on of surprise (see Appendix
\ref{reaction-after-experiment} in the supplementary material for an
overview of the reaction). In particular the two committees were only
around 50\% consistent about which papers were selected to appear at
the conference. Researchers realized, that if the review process had
been independently rerun, half the papers published at the conference
would have been different.

We explore these numbers further in three ways. First, we use the fact
that reviewer scores underwent a calibration process during the
conference. This process was focused on eliminating bias in reviewer
scale interpretation, but it also quantifies the subjectivity of
individual reviewer scores. Through a simulation study we demonstrate
that this subjectivity is the at the heart of the inconsistency. Second,
we explored whether these scores correlated with paper citation scores.
Taking citation scores as a proxy for paper impact,\footnote{There are
  problems with using citation scores as a way of assessing impact, see
  e.g. @Neylon-article09 for a discussion, but they have the advantage
  of being an objective, community driven measure and with seven years
  having passed since publication, the papers have had a chance to
  establish themselves.} We collected citation counts for each published
paper from Semantic Scholar,\footnote{\url{https://www.semanticscholar.org/}}
but found no correlation between paper quality scores and the paper's
eventual impact. Finally, we analyzed rejected papers from the
conference. We searched Semantic Scholar for papers with similar titles
by the same lead author in the literature allowing us to track the final
outlet for 680 papers that were rejected by the 2014 NeurIPS conference,
as well as their associated citation counts. For these papers we did
find correlation between quality scores and citation counts.

From these analyses we conclude that inconsistency in the conference
reviewing process is a consequence of the subjectivity in reviewer
assessments. And that in the high scoring range, reviewer quality scores
are not a good proxy for citation impact. However, reviewers seem better
at identifying weaker papers: low scoring papers were (on average) low
impact. Before discussing each of these areas, we first give a brief
reminder of the NeurIPS experiment.

\hypertarget{review-of-the-conference-and-the-experiment}{%
\subsection{Review of the Conference and the
Experiment}\label{review-of-the-conference-and-the-experiment}}

The NeurIPS conference is one of the premier conferences in machine
learning. In 2014 the conference was undergoing a period of rapid
growth. The conference was held in Montreal with 2,581 attendees at the
conference and associated workshops and tutorials. The papers presented
there have proven to be highly influential including breakthrough papers
in unsupervised learning with neural networks as well as papers on
sequence-to-sequence learning for machine translation. These papers have
gone on to have widespread societal impact.

At NeurIPS 2014, each paper was assigned to an Area Chair and at least
three reviewers. Final decisions about papers were made by video
conference between area chairs and the Program Chairs. For more details
on the reviewing process and the timelines involved see Appendix \ref{timeline-for-neurips} in
the supplementary material.

The Program Chairs of the 2014 conference decided to test the
consistency of the peer review process through a randomized experiment.
From the 1,678 submissions they chose 170 papers to undergo review by
two separate committees. Each committee was formed by separating the
reviewing body randomly into two groups, while the Area Chairs were
split manually to ensure proper coverage of expertise in the two bodies.
Each selected paper went through the review process independently.

The results of this process are summarized in Table \ref{table-neurips-experiment-results}, where the
\begin{table}[htb]
\caption{Table showing the results from the two committees as a confusion matrix. Four papers were rejected or withdrawn without review.}
\label{table-neurips-experiment-results}

\begin{tabular}{lc|c|c|}
& & \multicolumn{2}{c}{Committee 1} \\
& & Accept & Reject \\ \hline
\multirow{2}{*}{Committee 2} & Accept & 22 & 22 \\
& Reject & 21 & 101 
\end{tabular}
\end{table}

Cortes and Lawrence also looked at the correlation between the review scores across the two independent committees, a scatter plot of the scores from the committees is shown in Figure \ref{figure-calibrated-quality-correlation}, the Pearson correlation was computed as $\rho=0.55$. For details on the calibration process see Section \ref{reviewer-calibration-and-scoring-inconsistency}.

\begin{figure}[htb]
\includegraphics[width=0.9\columnwidth]{diagrams/neurips/calibrated-quality-correlation.pdf}

\caption{Correlation between calibrated reviewer scores across the two independent committees. Standard error on the correlation for $n=166$ papers and Gaussian assumptions is $s_r = 0.065$.}
\label{figure-calibrated-quality-correlation}
\end{figure}

During the experiment, the timing of submitted reviews was also tracked. There is evidence that reviews received after the submission deadline, were shorter, typically gave higher scores but with lower confidence (see Appendix \label{effect-of-late-reviews}), but there was insufficient power in the experiment to determine whether this had a significant effect on the correlation across the program committees. 

\section{Results}

Having given an overview of the experiment, we now follow up with our
three separate treatments of the results. First we will explore the
relationship between the conference calibration and the experimental
outcome.

\hypertarget{reviewer-calibration-and-scoring-inconsistency}{%
\subsection{Reviewer Calibration and Scoring Inconsistency}\label{reviewer-calibration-and-scoring-inconsistency}}

NeurIPS papers are evaluated by quality scores on a 10 point Likert
scale (see Appendix \ref{paper-scoring-and-reviewer-instructions}). A
classical challenge with such scales is that they may be interpreted
differently by different reviewers. Since the TK conference, NeurIPS
chairs have often calibrated reviewer scores using scripts of their
own devising. For example, John Platt who chaired the conference in
2006, used a regularized least squares model, this model is written up
in \cite{Platt-calibration12}. The year before us, Zoubin Ghaharamani
and Max Welling used a Bayesian extension of this model
\cite{Ge-bayesian15}. Corinna Cortes and Neil Lawrence also used a
Bayesian variant of the Platt-Burges model, but one that was
formulated as a Gaussian process. We give the details of this approach
in the supplementary material (Appendix \ref{reviewer-calibration}),
but in essence the core of the model is as follows. Each review score
is decomposed into three parts,
$$
y_{i,j} = f_i + b_j + \epsilon_{i, j},
$$
where $y_{i,j}$ is the score from the $j$th reviewer for the $i$th
paper. The score is then decomposed into $f_i$ which is the
\emph{objective} quality of the $i$th paper, i.e. it represents the
portion of the score that is common to all the reviewers. The term
$b_j$ is specific to the $j$th reviewer and it represents an offset or
bias associated with the $j$th reviewer. The idea being that different
reviewers interpret the scale differently. Finally $\epsilon_{i,j}$ is
a \emph{subjective} estimate of the quality of paper $i$ according to
reviewer $j$. It reflects how a specific reviewer's opinion differs
from other reviewers. These differences in opinion may be arising due
to differing expertise or perspective).

The model contains $n$ + $m$ + $n\hat{k}$ parameters where $n=1,678$
is the number of papers, $m=1,474$ is the number of reviewers and
$\hat{k}$ is the average number of reviewers per paper. Given that the
data consists of $n\hat{k}$ reviewing scores, the model is
over-parameterised. The original Platt-Burges model used
regularization to deal with this parameterisation, both
\cite{Ge-bayesian15} and Cortes and Lawrence deal with extra
parameters by allocating them a probability distribution. In the
Cortes and Lawrence case a Gaussian probability results in a latent
variable model that has a marginal likelihood which is jointly
Gaussian, so we have
$$
\mathbf{y} \sim N(\mu \mathbf{1}, \mathbf{K}),
$$
where $\mathbf{y}$ is a vector of stacked scores $\mathbf{1}$ is
the vector of ones and the elements of the covariance function are given
by
$$
k(i,j; k,l) = \delta_{i,k} \alpha_f + \delta_{j,l} \alpha_b + \delta_{i, k}\delta_{j,l} \sigma^2,
$$ where $i$ and $j$ are the index of one paper and reviewer and $k$
and $l$ are the index of a potentially different paper and
reviewer. Three of the parameters of this distribution, $\alpha_f$,
$\alpha_b$, $\sigma^2$ represent the explained variance of the the
score coming from objective quality rating, reviewer offset and
subjective quality rating respectively. As described in the appendix,
the calibrated reviewer score is estimated as the conditional density
of $f_i + \epsilon_{i,j}$. Note that the calibrated reviewer score
\emph{includes} the reviewer's \emph{subjective} opinion about the
paper. See Appendix \ref{reviewer-calibration-model} for more details
on the model as well as code for fitting the model in GPy
\cite{Gpy-20012}. The parameters of the fitted model are given in
Table \ref{fitted-calibration-parameters}.

\begin{table}[htb]
  \label{table-fitted-calibration-parameters}
  \caption{Fitted parameters of the calibration model. The parameters are very well determined as the model is based on around 6,000 reviewer scores. Once the individual reviewer offset, $\alpha_b=0.24$, is removed, the calibrated score $f_i = 1.28$ plus $\epsilon_{i,j}=1.27$ is made up approximately of subjective and objective assessment in roughly equal proportion.} 
  \begin{tabular}{ccc}
    \alpha_f & \alpha _b & \sigma^2 \\
    1.28 & 0.24 & 1.27
  \end{tabular}
\end{table}  

Under the model assumptions we see that calibrated review scores are
made up of subjective and objective opinion in roughly equal
proportions. In other words, 50\% of a typical reviewer's score is
coming from opinion that is particular to that reviewer and \emph{not}
shared with the other reviewers. This figure may seem large, but in
retrospect it is perhaps not surprising. Papers are judged by
subjective criteria such as novelty as well as more objective criteria
such as rigour. The subjectivity of reviewer scores also seems a
sensible starting point to unpick the inconsistency between the two
committees described by the NeurIPS experiment. 


The result is consistent with the correlation coefficient we computed
between the two independent committees, $\rho = 0.55 \pm 0.065$. Our
calibration model is suggesting that the overall correlation between
two committees would be given by $0.502 = 1.28/(1.28+1.27)$.

To check whether this subjective scoring also explains the
inconsistency in decisions between the two committees, we set up a
simple simulation study. For our simulations, we assumed that each
paper was scored according to the model we've given above and we
estimated the accept consistency through averaging across 100,000
samples. In Figure \ref{figure-consistency-vs-accept-rate} we show the
estimates of the accept consistency as a function of conference accept
rate. For three reviewers and 50\% subjectivity, the simulation
suggests that we should expect an accept consistency of around
63\%. This is higher than the accept consistency that we
observed, but it falls within the bounds of statistical error suggested e.g. by
a Bayesian analysis of the data (see Appendix \ref{bayesian-analysis}). Conceptually, the large error comes because although experimental sample size overall is a relatively healthy 166, the low accept rate for the conference means that the number of samples when exploring consistency across the two committees is around 40. This leads to a standard error of for our our estimate\footnote{Consider Committee 1 with $n=43$ accepts. Of these 21 are rejected by committee 2. Our estimate of the probability of inconsistency is given by $p=21/43$ with a standard error of $\sqrt{\frac{p(1-p)}{n}} = 0.076$.} of around 8\%.
Of course, the simulation model also oversimplifies the complexity of the final decision process, which involved detailed discussion of each papers between reviewers authors and program committee members. This simplification may be introducing some optimisitic bias in the simulation's estimate of the final accept precision. But it seems that our model simulation model is a plausible starting point for exporing the expected consistency of a given reviewing set up.

\begin{figure}[htb]
\includegraphics[width=0.90\columnwidth]{diagrams/neurips/accept-precision-vs-accept-rate.pdf}

\caption{Plot of the accept rate versus the accept consistency of the
  conference for 50\% subjectivity in a simulation of the conference
  with different numbers of reviewers per paper.}
\label{figure-consistency-vs-accept-rate}
\end{figure}

The simple simulation we describe suggests that a major source of inconsistency in the conference can be traced back to subjectivity in the reviews. Combination of the calibration model and our simulation suggests
an accept precision for the conference of around 63\%. This is consistent with the upper end of consistency estimates provided at the time of the experiment by Cortes and Lawrence. Their analysis suggested that the accept precision was was between 38\% and 64\% (see Appendix \ref{uncertainty-accept-rate}). This highlights the unreliability of the accept precision statistic. The statistical power is low because the number of samples used in its calculation is given by accept rate $\times$ experiment sample size.

\subsubsection{Consistency and Correctness}

It seems self-evident that we want greater consistency between review committees. After all, if decisions are inconsistent, then how can they be `correct'? While it's true that inconsistency implies incorrectness, the converse is not true. Consistency does not imply correctness. For example, if both committees were to choose papers to accept based on how many references they include, then their decisions would be consistent, but not correct. Given that we know that \emph{incorrect} decisions will be made, then we can also phrase the question in another way. Given that there will be errors, do we prefer errors which will be consistently made? When there are errors, variation in decision making may be a good thing: it could prevent a particular type of paper being consistently descriminated against.

We've established that there is inconsistency in the peer review process, and we have associated that inconsistency with subjective scoring by reviewers. But we have also warned against over emphasis on consistency as an aim for a reviewing process. Consistency is only a good thing if the decisions can also be shown to be correct. So, a follow up question would seem to be: how good is the committee at selecting the `right' papers? Unfortunately we don't have a ground truth assessment of what the right papers are. But, because time has passed between the conference and today, we can explore what happened to accepted papers in terms of their citation impact. 

\subsection{Impact of Accepted Papers}

Seven years have passed since the NeurIPS experiment and the papers published at the conference have had time to establish themselves. In this section we explore how they fared in terms of their \emph{citation impact}.\footnote{Citation counts certainly have flaws when being used as a measure of impact of a paper. They do not represent, e.g. adoption in industry, adoption for public policy and public awareness/education. However, they do provide a quantitative measure that allows us to analyze the impact of conference papers at scale.}

To determine the citation impact of papers, we searched for each accepted paper from the conference on Semantic Scholar.\footnote{See \href{https://www.semanticscholar.org/about}.} The Semantic Scholar ID of the papers was recorded and we made use of the Semantic Scholar API to retrieve the number of citing papers. The citation scores were transformed into the citation impact using a monotonic transformation as follows,
$$
\text{citation impact} = \log_{10} (1 + \text{number of citations}).
$$
This transformation eliminated the heavy tails of the distribution of citations, leading to a citation score distribution that is closer to a Gaussian, enabling us to make use of Pearson's $\rho$ for correlation measurement.

We computed the correlation between the average calibrated quality score and the citation impact. There was \emph{no significant correlation} between those scores. We show a scatter plot of the data in Figure \ref{figure-citations-vs-average-calibrated-quality-accept}. In the scatter plot we have added differential privacy noise to the values shown in the plot to obsfucate individual paper identities. Correlation coefficient is computed before adding the differential privacy noice (see Appendix \ref{correlation-of-quality-scores-and-citation}).

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.9\columnwidth]{diagrams/neurips/citations-vs-average-calibrated-quality-accept.pdf}
  \end{center}
  \caption{Scatter plot of the citation impact (defined as $\log_{10}(1+\text{citations})$) against the average calibrated quality score for accepted NeurIPS 2014 papers. To prevent reidentification of an individual paper's quality scores each point is corrupted by differentially private noise in the plot (correlation is computed before adding differentially private noise). We have also purposely left off the scale, as the main point in including the scatter plot is to show the general shape of the points, validating our use of Pearon's correlation coefficient, $\rho$.}
  \label{figure-citations-vs-average-calibrated-quality-accept}
\end{figure}

While the calibrated quality score is not specifically designed to measure impact. However, it may be surprising no correlation between this score and citation impact for the group of accepted papers. The implication that the quality score, which is the main criterion on which accept/reject decisions are being made, is uninformative in determining the paper's eventual influence should give significant pause for thought.

Does this mean that reviewers can't judge what papers are likely to be influential? Or is something else going on? In 2013 Welling and Ghaharamani introduced a separate scoring indicator (see Appendix \ref{impact-score}). Perhaps the answer to our quandry in the quality scores lies in the phrasing of the question they introduced.
\begin{quote}
  Independently of the Quality Score above, this is your opportunity to
identify papers that are very different, original, or otherwise
potentially impactful for the NIPS community.
\end{quote}

Here reviewers are being asked to judge the likely future impact of the paper. The score is a binary value, work is categorised as being either `potentially have a major impact' or `unlikely to have much impact'. Analysis of this score does show a statistically significant correlation with an accepted paper's citation impact.

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.9\columnwidth]{diagrams/neurips/citations-vs-average-impact-accept.pdf}
  \end{center}
  \caption{Scatter plot of the citation impact (defined as $\log_{10}(1+\text{citations})$) against the average impact score for accepted NeurIPS 2014 papers. As in Figure \ref{figure-citations-vs-average-calibrated-quality-accept}, data  is corrupted by differentially private noise in the plot. The scatter plot is to show the general shape of the points, validating our use of Pearon's correlation coefficient, $\rho$.}
  \label{figure-citations-vs-average-impact-accept}
\end{figure}

and we note in passing that the correlation for this score across the duplicated papers in the NeurIPS experiment was also relatively low: 0.27 (see Appendix \ref{correlation-of-duplicate-papers}).


\subsection{Fate of Rejected Papers}

\section{Conclusions}

\subsubsection*{Acknowledgements}


\bibliography{revisiting-the-neurips-experiment}

\onecolumn
\aistatstitle{Revisiting the NeurIPS Experiment: \\
Supplementary Materials}
\appendix
\include{the-neurips-experiment.include}

\vfill
\end{document}
