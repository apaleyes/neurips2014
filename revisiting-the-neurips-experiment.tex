\documentclass[twoside]{article}

\usepackage{aistats2021}
% If your paper is accepted, change the options for the package
% aistats2021 as follows:
%
%\usepackage[accepted]{aistats2021}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}
\usepackage{hyperref}

\usepackage{todonotes}
\usepackage[margin=2.5cm]{geometry}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{fancyvrb}
\usepackage{todo}



\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[
\aistatstitle{Inconsistency in Conference Peer Review: Revisiting the NeurIPS Experiment}
\aistatsauthor{Neil D. Lawrence \And Corinna Cortes}
\aistatsaddress{University of Cambridge \And  Google Research, New York}
]

\begin{abstract}
    In this paper we revisit the NeurIPS experiment that examined
inconsistency in conference peer review. We determine that the
underlying cause of inconsistency in the reviews was subjectivity in the
reviewers' quality scores: after score calibration 50\% of the variation
in reviewer scores was subjective in origin. Further, with seven years
passing since the experiment we find that for \emph{accepted} papers,
there is no correlation between quality scores and impact of the paper
as measured as a function of citation count. We trace the fate of
rejected papers, recovering where these papers were eventually
published. For these papers we find a correlation between quality scores
and impact. We conclude that the reviewing process for the 2014
conference was good for identifying poor papers, but poor for
identifying good papers. We give some suggestions for improving the
reviewing process but also warn against removing the subjective element.
Finally, we suggest that the real conclusion of the experiment is that
the community should place less onus on the notion of `top tier
conference publications' when assessing the quality of individual
researchers.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In 2014 the Program Chairs of the NeurIPS conference, Corinna Cortes
and Neil Lawrence, implemented the NeurIPS experiment. The experiment
was designed to assess the consistency of the conference peer
reviewing process. From the conference 10\% of the papers were
randomly chosen to be reviewed by two independent program
committees. The objective was to determine if decision making was
consistent across these two committees.  The results showed that the
decisions between the two committees were 74\% consistent as compared
with a random committee that would have been 62.5\% consistent. While
the review process can be shown to be significantly better than a
random committee (see Appendix \ref{a-random-committee-25} in the
supplementary material for an analysis), the reaction from some in the
community was on of surprise (see Appendix
\ref{reaction-after-experiment} in the supplementary material for an
overview of the reaction). In particular the two committees were only
around 50\% consistent about which papers were selected to appear at
the conference. Researchers realized, that if the review process had
been independently rerun, half the papers published at the conference
would have been different.

We explore these numbers further in three ways. First, we use the fact
that reviewer scores underwent a calibration process during the
conference. This process was focused on eliminating bias in reviewer
scale interpretation, but it also quantifies the subjectivity of
individual reviewer scores. Through a simulation study we demonstrate
that this subjectivity is the at the heart of the inconsistency. Second,
we explored whether these scores correlated with paper citation scores.
Taking citation scores as a proxy for paper impact,\footnote{There are
  problems with using citation scores as a way of assessing impact, see
  e.g. @Neylon-article09 for a discussion, but they have the advantage
  of being an objective, community driven measure and with seven years
  having passed since publication, the papers have had a chance to
  establish themselves.} We collected citation counts for each published
paper from Semantic Scholar,\footnote{\url{https://www.semanticscholar.org/}}
but found no correlation between paper quality scores and the paper's
eventual impact. Finally, we analyzed rejected papers from the
conference. We searched Semantic Scholar for papers with similar titles
by the same lead author in the literature allowing us to track the final
outlet for 680 papers that were rejected by the 2014 NeurIPS conference,
as well as their associated citation counts. For these papers we did
find correlation between quality scores and citation counts.

From these analyses we conclude that inconsistency in the conference
reviewing process is a consequence of the subjectivity in reviewer
assessments. And that in the high scoring range, reviewer quality scores
are not a good proxy for citation impact. However, reviewers seem better
at identifying weaker papers: low scoring papers were (on average) low
impact. Before discussing each of these areas, we first give a brief
reminder of the NeurIPS experiment.

\hypertarget{review-of-the-conference-and-the-experiment}{%
\subsection{Review of the Conference and the
Experiment}\label{review-of-the-conference-and-the-experiment}}

The NeurIPS conference is one of the premier conferences in machine
learning. In 2014 the conference was undergoing a period of rapid
growth. The conference was held in Montreal with 2,581 attendees at the
conference and associated workshops and tutorials. The papers presented
there have proven to be highly influential including breakthrough papers
in unsupervised learning with neural networks as well as papers on
sequence-to-sequence learning for machine translation. These papers have
gone on to have widespread societal impact.

At NeurIPS 2014, each paper was assigned to an Area Chair and at least
three reviewers. Final decisions about papers were made by video
conference between area chairs and the Program Chairs. For more details
on the reviewing process and the timelines involved see Appendix \ref{timeline-for-neurips} in
the supplementary material.

The Program Chairs of the 2014 conference decided to test the
consistency of the peer review process through a randomized experiment.
From the 1,678 submissions they chose 170 papers to undergo review by
two separate committees. Each committee was formed by separating the
reviewing body randomly into two groups, while the Area Chairs were
split manually to ensure proper coverage of expertise in the two bodies.
Each selected paper went through the review process independently.

The results of this process are summarized in Table \ref{table-neurips-experiment-results}, where the
\begin{table}[htb]
\caption{Table showing the results from the two committees as a confusion matrix. Four papers were rejected or withdrawn without review.}
\label{table-neurips-experiment-results}

\begin{tabular}{lc|c|c|}
& & \multicolumn{2}{c}{Committee 1} \\
& & Accept & Reject \\ \hline
\multirow{2}{*}{Committee 2} & Accept & 22 & 22 \\
& Reject & 21 & 101 
\end{tabular}
\end{table}

Cortes and Lawrence also looked at the correlation between the review scores across the two independent committees, a scatter plot of the scores from the committees is shown in Figure \ref{figure-calibrated-quality-correlation}, the Pearson correlation was computed as $\rho=0.55$. For details on the calibration process see Section \ref{reviewer-calibration-and-scoring-inconsistency}.

\begin{figure}[htb]
\includegraphics[width=0.60\textwidth]{diagrams/neurips/calibrated-quality-correlation.pdf}

\caption{Correlation between calibrated reviewer scores across the two independent committees. Standard error on the correlation for $n=166$ papers and Gaussian assumptions is $s_r = 0.065$.}
\label{figure-calibrated-quality-correlation}
\end{figure}

During the experiment, the timing of submitted reviews was also tracked. There is evidence that reviews received after the submission deadline, were shorter, typically gave higher scores but with lower confidence (see Appendix \label{effect-of-late-reviews}), but there was insufficient power in the experiment to determine whether this had a significant effect on the correlation across the program committees. 

\section{Results}

Having given an overview of the experiment, we now follow up with our three separate treatments of the results. First we will explore the relationship between the conference calibration and the experimental outcome.

\hypertarget{reviewer-calibration-and-scoring-inconsistency}{%
\subsection{Reviewer Calibration and Scoring Inconsistency}\label{reviewer-calibration-and-scoring-inconsistency}}

NeurIPS papers are evaluated by quality scores on a 10 point Likert
scale (see Appendix \ref{paper-scoring-and-reviewer-instructions}). A
classical challenge with such scales is that they may be interpreted
differently by different reviewers. Since the TK conference, NeurIPS
chairs have often calibrated reviewer scores using scripts of their
own devising. For example, John Platt who chaired the conference in
2006, used a regularized least squares model, this model is written up
in \cite{Platt-calibration12}. The year before us, Zoubin Ghaharamani
and Max Welling used a Bayesian extension of this model
\cite{Ge-bayesian15}. Corinna Cortes and Neil Lawrence also used a
Bayesian variant of the Platt-Burges model, but one that was
formulated as a Gaussian process. We give the details of this approach
in the supplementary material (Appendix \ref{reviewer-calibration}),
but in essence the core of the model is as follows. Each review score
is decomposed into three parts,
$$
y_{i,j} = f_i + b_j + \epsilon_{i, j},
$$
where $y_{i,j}$ is the score from the $j$th reviewer for the $i$th
paper. The score is then decomposed into $f_i$ which is the
\emph{objective} quality of the $i$th paper, i.e. it represents the
portion of the score that is common to all the reviewers. The term
$b_j$ is specific to the $j$th reviewer and it represents an offset or
bias associated with the $j$th reviewer. The idea being that different
reviewers interpret the scale differently. Finally $\epsilon_{i,j}$ is
a \emph{subjective} estimate of the quality of paper $i$ according to
reviewer $j$. It reflects how a specific reviewer's opinion differs
from other reviewers. These differences in opinion may be arising due
to differing expertise or perspective).

The model contains $n$ + $m$ + $n\hat{k}$ parameters where $n=1,678$
is the number of papers, $m=1,474$ is the number of reviewers and
$\hat{k}$ is the average number of reviewers per paper. Given that the
data consists of $n\hat{k}$ reviewing scores, the model is
over-parameterised. The original Platt-Burges model used
regularization to deal with this parameterisation, both
\cite{Ge-bayesian15} and Cortes and Lawrence deal with extra
parameters by allocating them a probability distribution. In the
Cortes and Lawrence case a Gaussian probability results in a latent
variable model that has a marginal likelihood which is jointly
Gaussian, so we have
$$
\mathbf{y} \sim N(\mu \mathbf{1}, \mathbf{K}),
$$
where $\mathbf{y}$ is a vector of stacked scores $\mathbf{1}$ is
the vector of ones and the elements of the covariance function are given
by
$$
k(i,j; k,l) = \delta_{i,k} \alpha_f + \delta_{j,l} \alpha_b + \delta_{i, k}\delta_{j,l} \sigma^2,
$$ where $i$ and $j$ are the index of one paper and reviewer and $k$
and $l$ are the index of a potentially different paper and
reviewer. Three of the parameters of this distribution, $\alpha_f$,
$\alpha_b$, $\sigma^2$ represent the explained variance of the the
score coming from objective quality rating, reviewer offset and
subjective quality rating respectively. As described in the appendix,
the calibrated reviewer score is estimated as the conditional density
of $f_i + \epsilon_{i,j}$. Note that the calibrated reviewer score
\emph{includes} the reviewer's \emph{subjective} opinion about the
paper. See Appendix \ref{reviewer-calibration-model} for more details
on the model as well as code for fitting the model in GPy
\cite{Gpy-20012}. The parameters of the fitted model are given in
Table \ref{fitted-calibration-parameters}.

\begin{table}[htb]
  \label{table-fitted-calibration-parameters}
  \caption{Fitted parameters of the calibration model. The parameters are very well determined as the model is based on around 6,000 reviewer scores. Once the individual reviewer offset, $\alpha_b=0.24$, is removed, the calibrated score $f_i = 1.28$ plus $\epsilon_{i,j}=1.27$ is made up approximately of subjective and objective assessment in roughly equal proportion.} 
  \begin{tabular}{ccc}
    \alpha_f & \alpha _b & \sigma^2 \\
    1.28 & 0.24 & 1.27
  \end{tabular}
\end{table}  

Under the model assumptions we see that calibrated review scores are
made up of subjective and objective opinion in roughly equal
proportions. In other words, 50\% of a typical reviewer's score is
coming from opinion that is particular to that reviewer and \emph{not}
shared with the other reviewers. This figure may seem large, but in
retrospect it is perhaps not surprising. Papers are judged by
subjective criteria such as novelty as well as more objective criteria
such as rigour. The subjectivity of reviewer scores also seems a
sensible starting point to unpick the inconsistency between the two
committees described by the NeurIPS experiment. 


The result is
consistent with the correlation coefficient we computed between the
two independent committees, $\rho = 0.55 \pm 0.065$. Our calibration
model is suggesting that the overall correlation between two
committees would be given by $0.502 = 1.28/(1.28+1.27)$.

To check whether this subjective scoring also explains the
inconsistency in decisions between the two committees, we set up a
simple simulation study. For our simulations, we assumed that each
paper was scored according to the model we've given above and we
estimated the accept consistency through averaging across 100,000
samples. In Figure \ref{figure-consistency-vs-accept-rate} we show the
estimates of the accept consistency as a function of conference accept
rate. For four reviewers and 50\% subjectivity, the simulation
suggests that we should expect an accept consistency of around
68\%. This is higher than the accept consistency that we observed, but
given the relatively small number of accepted papers in the
experiment, it falls well within binomial error bars.

\begin{figure}[htb]
\includegraphics[width=0.50\textwidth]{diagrams/neurips/accept-precision-vs-accept-rate.pdf}

\caption{Plot of the accept rate versus the accept consistency of
the conference for 50\% subjectivity in a simulation of the conference with different numbers of reviewers per paper.}
\label{figure-consistency-vs-accept-rate}
\end{figure}

If we accept the model used for the calibration and the simulation is capturing the underlying phenomenon, then the source of inconsistency in the conference can be traced back to subjectivity in the reviews. Further the model shows that the actual accept consistency of the conference may have been higher, at around 68\%. This is consistent with the upper end of an analysis of the experiment performed by Cortes and Lawrence at the time which suggested that the consistent accept rate was between 38\% and 64\% (see Appendix \ref{uncertainty-accept-rate}). Note this mainly highlights the unreliability of the consistent accepts summary measure. Power for this measure is reduced because the number of samples used in its calculation is given by accept rate $\times$ experiment sample size.

Regardless, it seems self-evident that we might want greater consistency between review committees. After all, if decisions are inconsistent, then how can they be `correct'? While it's true that inconsistency implies incorrectness, the converse is not true. Consistency does not imply correctness. For example, if both committees were to choose papers to accept based on how many references they include, then their decisions would be consistent, but not correct. Given that we know that \emph{incorrect} decisions will be made, then we can also phrase the problem in another way. Given that there are errors, do we prefer those errors to be consistently made? In the presence of errors, we might argue that variation in decision making is a good thing: it prevents particular types of paper being consistently descriminated against.

We've established that there is inconsistency in the peer review process, and we have associated that inconsistency with subjective scoring by reviewers. But we have argued that consistency is only a good thing if the decisions can also be shown to be correct. So, a follow up question would seem to be: how good is the committee at selecting the `right' papers? Unfortunately we don't have a ground truth assessment of what the right papers are. But, because time has passed between the conference and today, we can explore what happened to accepted papers in terms of their citation impact. 

\subsection{Impact of Accepted Papers}

Seven years have passed since the NeurIPS experiment and the papers published at the conference have had time to establish themselves. In this section we explore how they fared in terms of their \emph{citation impact}.

To determine the citation impact of papers, we searched for each accepted paper from the conference on Semantic Scholar. The Semantic Scholar ID of the papers was recorded and we made use of the Semantic Scholar API to retrieve the number of citing papers. The citation scores were transformed into the citation impact using
$$
\text{citation impact} = \log_{10} (1 + \text{number of citations})
$$
which by visual inspection rendered citation scores roughly Gaussian distributed, enabling us to make use of Pearson's rho for correlation measurement.

Our first measurement was to compute the correlation between calibrated quality score and the citation impact. We found \emph{no significant correlation} between those scores. In Figure \ref{figure-citations-vs-average-calibrated-quality-accept} we show a scatter plot of paper calibrated quality scores against the citation impact. In the scatter plot we added differential privacy noise to the values shown in the plot to obsfucate individual paper identities. Correlation coefficient is computed before adding the differential privacy noice (see Appendix \ref{correlation-of-quality-scores-and-citation}).

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.70\textwidth]{diagrams/neurips/citations-vs-average-calibrated-quality-accept.pdf}
  \end{center}
  \caption{Scatter plot of the citation impact (defined as $\log_{10}(1+\text{citations})$) against the average calibrated quality score for accepted NeurIPS 2014 papers. To prevent reidentification of individual papers quality scores and citation count, each point is corrupted by differentially private noise in the plot (correlation is computed before adding differentially private noise). We have also purposely left off the scale, as the main point in including the scatter plot is to show the general shape of the points, validating our use of Pearon's correlation coefficient, $\rho$.}
  \label{figure-citations-vs-average-calibrated-quality-accept}
\end{figure}

The calibrated quality score is not specifically designed to measure impact. However, it may be surprising to some readers that there is no correlation between the score and citation impact for the group of accepted papers. The implication that the quality score, which is the main criterion on which accept/reject decisions are being made, may strike us as odd. 

\subsection{Fate of Rejected Papers}

\section{Conclusions}

\subsubsection*{Acknowledgements}


\bibliography{revisiting-the-neurips-experiment}

\onecolumn
\aistatstitle{Revisiting the NeurIPS Experiment: \\
Supplementary Materials}
\appendix
\include{the-neurips-experiment.include}

\vfill
\end{document}
