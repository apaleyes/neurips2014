{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Bidding Lists for Reviewers\n",
    "\n",
    "### 11th June 2014 Neil D. Lawrence\n",
    "\n",
    "#### Modified May 2021 to use new cmtutils\n",
    "\n",
    "This notebook loads in the TPMS scores and key word similarities and and allocates the highest similarity score matches to reviewers for bidding on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmtutils.cmtutils as cu\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, we need to get all the current information out of CMT. That includes: external matching scores, conflict information, keyword overlap. We do this from `Assignments & Conflicts > *** > Automatic Assignment Wizard` where `***` is either reviewers or meta reviewers. Here's the [link for meta-reviewers](https://cmt.research.microsoft.com/NIPS2014/Protected/Chair/MetaReviewerAutoAssignments.aspx). Proceed through the wizard putting in some values. Then at the end click on `Export Data for Custom Assignment`. You will need to select: `Subject Areas: Paper and Meta-Reviewer`, `Toronto Paper Matching System` and `Conflicts` for setting things up for bidding. For setting things up for final allocation you also need `bids`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewer and Meta Reviewer Bids\n",
    "https://cmt.research.microsoft.com/NIPS2014/Protected/Chair/ManageAssignmentsExport.aspx?data=bids&view=cs&format=tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPMS Reviewer Matching Scores\n",
    "First you need to download the reviewer matching scores.\n",
    "\n",
    "https://cmt.research.microsoft.com/NIPS2014/Protected/Chair/ManageAssignmentsExport.aspx?data=externalmatching&view=cs&format=tab&excludemetareviewer=1&serviceid=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load in the external matching scores.\n",
    "\n",
    "filename = '2014-06-19_externalMatchingScores.tsv'\n",
    "filename=os.path.join(cu.cmt_data_directory, filename)\n",
    "affinity = pd.read_csv(filename, delimiter='\\t', \n",
    "                       index_col='PaperID', \n",
    "                       na_values=['N/A']).fillna(0)\n",
    "#data = cu.xl_read(, index_col='Paper ID', dataframe=True)\n",
    "#affinity = data.items\n",
    "# Scale affinities to be between 0 and 1.\n",
    "affinity -= affinity.values.min()\n",
    "affinity /= affinity.values.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Subject Areas\n",
    "\n",
    "Now load in paper subject areas and group them by the Paper ID. This file is downloaded from:\n",
    "\n",
    "https://cmt.research.microsoft.com/NIPS2014/Protected/Chair/ManageAssignmentsExport.aspx?data=subjectareas&view=cs&format=excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load in paper subject areas\n",
    "filename = '2014-06-13_paperSubjectAreas.xls'\n",
    "data = cu.xl_read(filename=os.path.join(cu.cmt_data_directory, filename), \n",
    "                  index_col='Selected Subject Area', \n",
    "                  header=2,\n",
    "                  dataframe=True, \n",
    "                  worksheet_number=1)\n",
    "paper_subject = data.items.groupby(by=['Paper ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewer Subject Areas\n",
    "\n",
    "Load in reviewer (or meta reviewer) subject areas and group them by email. This file is downloaded from:\n",
    "\n",
    "https://cmt.research.microsoft.com/NIPS2014/Protected/Chair/ManageAssignmentsExport.aspx?data=subjectareas&view=cr&format=excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load in (meta-)reviewer subject areas\n",
    "filename = '2014-06-13_reviewerSubjectAreas.xls'\n",
    "data = cu.xl_read(filename=os.path.join(cu.cmt_data_directory, filename), \n",
    "                  index_col='Selected Subject Area', \n",
    "                  header=2,\n",
    "                  dataframe=True, \n",
    "                  worksheet_number=1)\n",
    "reviewer_subject = data.items.groupby(by=['Email'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Assignments and Conflicts\n",
    "\n",
    "Possible assignments is derived from the conflicts. It lists the people that the paper *could* be assigned to. This file is downloaded from:\n",
    "\n",
    "https://cmt.research.microsoft.com/NIPS2014/Protected/Chair/ManageAssignmentsExport.aspx?data=possibleassignments&view=cs&format=tab&excludemetareviewer=1\n",
    "\n",
    "Conflicts is downloaded from:\n",
    "\n",
    "https://cmt.research.microsoft.com/NIPS2014/Protected/Chair/ManageAssignmentsExport.aspx?data=conflicts&view=cs&format=tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # Read from the TSV format CMT provide.\n",
    "    filename = 'Conflicts.txt'\n",
    "    with open(os.path.join(cu.cmt_data_directory, filename)) as fin:\n",
    "        rows = ( line.strip().split('\\t') for line in fin)\n",
    "        conflicts_groups = { row[0]:row[1:] for row in rows}\n",
    "    papers = conflicts_groups.keys()\n",
    "    conflicts_by_reviewer = {}\n",
    "\n",
    "    for paper in papers:\n",
    "        for reviewer in conflicts_groups[paper]:\n",
    "            if reviewer in conflicts_by_reviewer:\n",
    "                conflicts_by_reviewer[reviewer].append(paper)\n",
    "            else:\n",
    "                conflicts_by_reviewer[reviewer] = [paper]\n",
    "    conflicts_file = True\n",
    "else:\n",
    "    # And finally we load in 'possible assignments'\n",
    "    filename = '2014-06-13_possibleAssignmentsByPaper.xls'\n",
    "    data = cu.xl_read(filename=os.path.join(cu.cmt_data_directory, filename), \n",
    "                      index_col='Paper ID',\n",
    "                      header=2,\n",
    "                      dataframe=True)\n",
    "    possible_assignments = data.items\n",
    "    regex = re.compile(r'\\(([^)]*)\\)')\n",
    "    papers = possible_assignments.index\n",
    "    conflicts_file = False\n",
    "    #conflicts = conflicts.set_index('Reviewer/Meta-Reviewer')\n",
    "#conflicts_groups = conflicts.groupby('PaperID').groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a simple similarity based on subject overlap. The similarity is the number of overlapping keywords divided by the square root of the number of reviewer keywords multiplied by the square root of the number of subject keywords. `None of the above` is removed as  a term if it is present.\n",
    "\n",
    "This actually turns out not to be a very sensible way of doing it. I was only just getting used to pandas when I wrote this. There's a more sensible (much faster) way of getting these similarities out in the [reviewer calibration notebook](./Reviewer Calibration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_sim = pd.DataFrame(np.zeros((len(paper_subject.groups), len(reviewer_subject.groups))), \n",
    "                           index=paper_subject.groups, columns=reviewer_subject.groups)\n",
    "for paper in paper_subject.groups:        \n",
    "    set_paper = set(paper_subject.groups[paper]) - set(['None of the above'])\n",
    "    for reviewer in reviewer_subject.groups:\n",
    "        set_reviewer = set(reviewer_subject.groups[reviewer]) - set(['None of the above'])\n",
    "        if len(set_paper)>0 and len(set_reviewer)>0:\n",
    "            norm = np.sqrt(len(set_paper))*np.sqrt(len(set_reviewer))\n",
    "        else:\n",
    "            norm = 1. # don't normalise if the vector is all zeros!\n",
    "        subject_sim.loc[paper, reviewer] = len(set_reviewer & set_paper)/norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight $\\alpha$ portion of the affinities and $1-\\alpha$ of the keyword similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate to Top 40 High Scoring Reviewers\n",
    "\n",
    "A little big of background is needed here. At the time the code was written Corinna and I were struggling to get CMT to perform an allocation. It was across the weekend so there was no support, and it turned out the scale of the 2014 NIPS had broken a few different things. This caused me to start writing paper allocation code, within the space of a few days, without having much knowledge of the literature. This first piece of code simply allocates each paper to the top 40 high scoring reviewers. It is superceded by the code that follows. The code that follows ranks the entire matrix and starts by allocating to the highest score in the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.5\n",
    "assignment = {}\n",
    "all_reviewers = affinity.columns\n",
    "for reviewer in all_reviewers:\n",
    "    assignment[reviewer] = []\n",
    "assignment_paper = {}\n",
    "\n",
    "all_scores = (alpha*affinity + (1-alpha)*subject_sim)\n",
    "min_vals = all_scores.min()\n",
    "max_vals = all_scores.max()\n",
    "normalise_scores = True\n",
    "\n",
    "\n",
    "for paper_str in papers:\n",
    "    paper = int(paper_str)\n",
    "    if conflicts_file:\n",
    "        reviewers = set(all_reviewers) - set(conflicts_groups[paper_str])\n",
    "    else:\n",
    "        reviewers = regex.findall(possible_assignments['Assigned Meta-Reviewers'][paper])\n",
    "        assert(len(reviewers)==int(possible_assignments['Number of Meta-Reviewers'][paper]))\n",
    "    scores = (1-alpha)*subject_sim.loc[paper].reindex(reviewers)\n",
    "    if paper in affinity.index:\n",
    "        scores += alpha*affinity.loc[paper].reindex(reviewers)\n",
    "    else:\n",
    "        print(\"Warning paper \", paper, \" not found in TPMS scores.\")\n",
    "    \n",
    "    if normalise_scores:\n",
    "        scores -= min_vals[reviewers]\n",
    "        scores/=(max_vals-min_vals)[reviewers]\n",
    "        #print scores\n",
    "    scores.sort_values(ascending=False)\n",
    "    assignment_paper[paper] = scores[:40].index\n",
    "    for reviewer in assignment_paper[paper]:\n",
    "        assignment[reviewer].append(paper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code if you loaded in the `conflicts.xls` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = (alpha*affinity + (1-alpha)*subject_sim)\n",
    "min_vals = all_scores.min()\n",
    "max_vals = all_scores.max()\n",
    "normalise_scores = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   2,    4,    6,    7,    8,    9,   10,   11,   13,   18,\n",
       "            ...\n",
       "            1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949],\n",
       "           dtype='int64', length=1831)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify conflicts by setting to -1\n",
    "rank_scores = all_scores.copy()\n",
    "for paper in conflicts_groups:\n",
    "    series = rank_scores.loc[int(paper)].reindex(conflicts_groups[paper])\n",
    "    series[conflicts_groups[paper]] = -1.\n",
    "    rank_scores.loc[int(paper)] = series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking All Scores\n",
    "\n",
    "After some thought, this next piece of code was preferred. Now all scores are taken and ranked. The papers then can be allocated from the most similar paper-reviewer pair and downwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_vec = rank_scores.reset_index()\n",
    "score_vec = pd.melt(score_vec, id_vars=['index'])\n",
    "#score_vec = score_vec[score_vec.value != -1.]\n",
    "score_vec = score_vec[score_vec.value > 0.1]\n",
    "score_vec = score_vec[pd.notnull(score_vec.value)]\n",
    "score_vec.columns = ['PaperID', 'Email', 'Score']\n",
    "score_vec = score_vec.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_number_assigned = {}\n",
    "reviewer_number_assigned = {}\n",
    "max_number_paper = 17\n",
    "max_number_reviewer = 25\n",
    "assignment_paper = {}\n",
    "assignment_reviewer = {}\n",
    "\n",
    "for idx in score_vec.index:\n",
    "    paper = str(score_vec['PaperID'][idx])\n",
    "    assign = True\n",
    "    if paper in paper_number_assigned:\n",
    "        if paper_number_assigned[paper]>=max_number_paper:\n",
    "            assign = False\n",
    "            continue\n",
    "    else:\n",
    "        paper_number_assigned[paper] = 0\n",
    "\n",
    "    reviewer = str(score_vec['Email'][idx])\n",
    "    if reviewer in reviewer_number_assigned:\n",
    "        if reviewer_number_assigned[reviewer]>=max_number_reviewer:\n",
    "            assign = False\n",
    "            continue\n",
    "    else:\n",
    "        reviewer_number_assigned[reviewer] = 0\n",
    "    \n",
    "    if assign:\n",
    "        if paper in assignment_paper:\n",
    "            assignment_paper[paper].append(reviewer)\n",
    "        else:\n",
    "            assignment_paper[paper] = [reviewer]\n",
    "        \n",
    "        if reviewer in assignment_reviewer:\n",
    "            assignment_reviewer[reviewer].append(paper)\n",
    "        else:\n",
    "            assignment_reviewer[reviewer] = [paper]\n",
    "        paper_number_assigned[paper] += 1\n",
    "        reviewer_number_assigned[reviewer] += 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FInd Reviewers with Less than 25 Papers\n",
    "\n",
    "Now reviewers who haven't got a full allocation of 25 papers to rank are allocated a top up number of papers. In later runs of the allocation algorithm, papers were allocated in batches to reviewers (each reviewer allocated up to 5, then to 10, then to 20 etc.) to balance things a little more. But in this early stage allocation to get the bidding going it was done in this 'top up' style way. Due to the problems with CMT and the allocation steps being unforeseen, we felt quite a lot of time pressure at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning reviewer  ivor.tsang@gmail.com  not found in subject similarities.\n",
      "Warning reviewer  jason@sccn.ucsd.edu  not found in subject similarities.\n",
      "Warning reviewer  jbpoline@gmail.com  not found in subject similarities.\n",
      "Warning reviewer  nathan@robots.ox.ac.uk  not found in subject similarities.\n",
      "Warning reviewer  or.zuk@mail.huji.ac.il  not found in subject similarities.\n",
      "Warning reviewer  tsainath@gmail.com  not found in subject similarities.\n",
      "Warning reviewer  wenzheng.ee@gmail.com  not found in subject similarities.\n",
      "Warning reviewer  xj@cs.princeton.edu  not found in subject similarities.\n",
      "Warning reviewer  yuekai@gmail.com  not found in subject similarities.\n"
     ]
    }
   ],
   "source": [
    "all_papers = affinity.index\n",
    "min_papers = 25\n",
    "additional_papers = {}\n",
    "additional_reviewers = {}\n",
    "additional_number_assigned = []\n",
    "for reviewer in affinity.columns:\n",
    "    if reviewer in reviewer_number_assigned:\n",
    "        num_papers = reviewer_number_assigned[reviewer] \n",
    "        if num_papers < min_papers:\n",
    "            required_papers = min_papers - num_papers\n",
    "        elif reviewer == 'ventura@cs.byu.edu':\n",
    "            required_papers = 25\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        required_papers = min_papers\n",
    "    \n",
    "    if reviewer in conflicts_by_reviewer:\n",
    "        papers = set(all_papers) - set(conflicts_by_reviewer[reviewer])\n",
    "    else:\n",
    "        papers = set(all_papers)\n",
    "    scores = alpha*affinity.loc[papers][reviewer]\n",
    "    if reviewer in subject_sim.columns:\n",
    "        scores += (1-alpha)*subject_sim.loc[papers][reviewer]\n",
    "    else:\n",
    "        print(\"Warning reviewer \", reviewer, \" not found in subject similarities.\")\n",
    "   \n",
    "    scores.sort_values(ascending=False, inplace=True)\n",
    "    additional_reviewers[reviewer] = scores[:required_papers].index\n",
    "    for paper in additional_reviewers[reviewer]:\n",
    "        if str(paper) in additional_papers:\n",
    "            additional_papers[str(paper)].append(reviewer)\n",
    "        else:\n",
    "            additional_papers[str(paper)] = [reviewer]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit of code writes the allocation for sharing with Corinna, just for hand checking to ensure that something sensible is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cu.cmt_data_directory, 'reviewer_bidding_allocation.txt'), 'w') as f:\n",
    "    for reviewer in assignment_reviewer:\n",
    "        f.write('Reviewer ' + reviewer + '\\n')\n",
    "        f.write('\\n')\n",
    "        for paper in assignment_reviewer[reviewer]:\n",
    "            f.write(str(paper) + \" \" + \"https://cmt.research.microsoft.com/NIPS2014/Protected/Chair/ViewSubmissionDetails.aspx?paperId=\" + str(paper) + '\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was for writing the export file for CMT to load in the bidding allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cu.cmt_data_directory, 'reviewer_bidding_allocation.tsv'), 'w') as f:\n",
    "    for reviewer in assignment_reviewer:\n",
    "        for paper in assignment_reviewer[reviewer]:\n",
    "            f.write(', '.join([reviewer, str(paper)]) + '\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is similar, but uses the CMT XML format which they find easier to load in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cu.cmt_data_directory, 'reviewer_assignments.xml'), 'w') as f:\n",
    "    f.write('<assignments>\\n')\n",
    "    for paper in assignment_paper:\n",
    "        f.write('  <submission submissionId=\"' + paper + '\">\\n')\n",
    "        for reviewer in assignment_paper[paper]:\n",
    "            f.write('    <reviewer email=\"' + reviewer + '\"/>\\n')\n",
    "        f.write('  </submission>\\n')\n",
    "    f.write('</assignments>\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cu.cmt_data_directory, 'additional_reviewer_bidding_allocation.tsv'), 'w') as f:\n",
    "    for reviewer in additional_reviewers:\n",
    "        for paper in additional_reviewers[reviewer]:\n",
    "            f.write(', '.join([reviewer, str(paper)]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_val = ''\n",
    "for reviewer in additional_reviewers:\n",
    "    if len(str_val) >0:\n",
    "        str_val += ';' + reviewer\n",
    "    else:\n",
    "        str_val = reviewer\n",
    "print(str_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1388"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list(additional_reviewers.keys()) + list(assignment_reviewer.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Extra Reviewers\n",
    "\n",
    "Some reviewers complained that they weren't seeing enough papers in their area. Most of these reviewers had many secondary subject areas. The similarity measure being used above (mainly for the purposes of speed) was originally not weighting primary key differently from the secondary keys. This meant reviewers with many secondary keys were getting a lot of papers not in their core area. In this next section of code we added additional papers to reviewers for bidding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "all_papers = affinity.index\n",
    "min_papers = 25\n",
    "additional_papers = {}\n",
    "additional_reviewers = {}\n",
    "additional_number_assigned = []\n",
    "\n",
    "for reviewer in reviewers:\n",
    "    required_papers = 25\n",
    "    if reviewer in conflicts_by_reviewer:\n",
    "        papers = set(all_papers) - set(conflicts_by_reviewer[reviewer])\n",
    "    else:\n",
    "        papers = set(all_papers)\n",
    "    if reviewer in subject_sim.loc[papers]:\n",
    "        scores = (1-alpha)*subject_sim.loc[papers][reviewer]\n",
    "    else:\n",
    "        scores = np.NaN\n",
    "    if reviewer in affinity.columns:\n",
    "        scores += alpha*affinity.loc[papers][reviewer]\n",
    "    else:\n",
    "        print(\"Warning reviewer \", reviewer, \" not found in TPMS scores.\")\n",
    "\n",
    "    scores.sort_values(ascending=False, inplace=True)\n",
    "    additional_reviewers[reviewer] = scores[:required_papers].index\n",
    "    for paper in additional_reviewers[reviewer]:\n",
    "        if str(paper) in additional_papers:\n",
    "            additional_papers[str(paper)].append(reviewer)\n",
    "        else:\n",
    "            additional_papers[str(paper)] = [reviewer]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cu.cmt_data_directory, 'further_additional_reviewer_bidding_allocation.tsv'), 'w') as f:\n",
    "    for reviewer in additional_reviewers:\n",
    "        for paper in additional_reviewers[reviewer]:\n",
    "            f.write(', '.join([reviewer, str(paper)]) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
