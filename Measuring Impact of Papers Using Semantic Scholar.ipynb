{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3c8bb1",
   "metadata": {},
   "source": [
    "# Measure Impact using Semantic Scholar\n",
    "\n",
    "### Neil D. Lawrence 7th June 2021\n",
    "\n",
    "This notebook looks at the actual impact of the papers published using the Semantic Scholar data base for tracking citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4602a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmtutils as cu\n",
    "import cmtutils.nipsy as nipsy\n",
    "import cmtutils.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461039cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = cu.Papers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da34d8d6",
   "metadata": {},
   "source": [
    "https://proceedings.neurips.cc/paper/2014 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UPDATE_IMPACTS = False # Set to True to download impacts from Semantic Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671a26f",
   "metadata": {},
   "source": [
    "The impact of the different papers is downloaded from Semantic scholar using their REST API. This can take some time, and they also throttle the calls. At the moment the code below deosn't handle the throttling correctly. However, you it will load the cached version of of citations scores from the given date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPDATE_IMPACTS:\n",
    "    from datetime import datetime\n",
    "    date=datetime.today().strftime('%Y-%m-%d')\n",
    "else:\n",
    "    date = \"2021-06-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun to download impacts from Semantic Scholar\n",
    "if UPDATE_IMPACTS:\n",
    "    semantic_ids = nipsy.load_semantic_ids()\n",
    "    citations_dict = citations.to_dict(orient='index')\n",
    "    # Need to be a bit cleverer here. Semantic scholar will throttle this call.\n",
    "    sscholar = nipsy.download_citation_counts(citations_dict=citations_dict, semantic_ids=semantic_ids)\n",
    "    citations = pd.DataFrame.from_dict(citations_dict, orient=\"index\") \n",
    "    citations.to_pickle(date + '-semantic-scholar-info.pickle')\n",
    "else: \n",
    "    citations = nipsy.load_citation_counts(date=date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c7e41",
   "metadata": {},
   "source": [
    "The final decision sheet provides information about what happened to all of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = nipsy.load_decisions()\n",
    "nipsy.augment_decisions(decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fdf086",
   "metadata": {},
   "source": [
    "This is joined with the citation information to provide our main ability to understand the impact of these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joindf = nipsy.join_decisions_citations(decisions, citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937abc2c",
   "metadata": {},
   "source": [
    "## Correlation of Quality Scores and Citation\n",
    "\n",
    "Our first study will be to check the correlation between quality scores of papers and how many times that the papers have been cited in practice. In the plot below, rejected papers are given as crosses, accepted papers are given as dots. We include all papers, whether published in a venue or just available through ArXiv or other preprint servers. We show the published/non-published quality scores and $\\log_{10}(1+\\text{citations})$ for all papers in the plot below. In the plot we are showing each point corrupted by some Laplacian noise and also removing axes. The idea is to give a sense of the distribution rather than reveal the score of a particular paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai as ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca69d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"average_calibrated_quality\"\n",
    "filter_col = \"all\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ax.set_xticks([])\n",
    "ma.write_figure(filename=\"citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1c543",
   "metadata": {},
   "source": [
    "The correlation seems strong, but of course, we are looking at papers which were accepted and rejected by the conference. This is dangerous, as it is quite likely that presentation at the conference may provide some form of lift to the papers' numbers of citations. So, the right thing to do is to look at the groups separately. \n",
    "\n",
    "Looking at the accepted papers only shows a very different picture. There is very little correlation between accepted papers' quality scores and the number of citations they receive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4828f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"average_calibrated_quality\"\n",
    "filter_col = \"accept\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ma.write_figure(filename=\"citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661fccc4",
   "metadata": {},
   "source": [
    "Conversely, looking at rejected papers only, we do see a slight trend, with higher scoring papers achieving more citations on average. This, combined with the lower average number of citations in the rejected paper group, alongside their lower average scores, explains the correlation we originally observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939aaa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"average_calibrated_quality\"\n",
    "filter_col = \"reject\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ma.write_figure(filename=\"citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39beac",
   "metadata": {},
   "source": [
    "Welling and Ghahramani introduced an \"impact\" score in NeurIPS 2013, we might expect the impact score to show correlation. And indeed, despite the lower range of the score (a reviewer can score either 1 or 2) we do see *some* correlation, although it is relatively weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99869c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"average_impact\"\n",
    "filter_col = \"accept\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ma.write_figure(filename=\"citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08708dd",
   "metadata": {},
   "source": [
    "Finally, we also looked at correlation between the *confidence* score and the impact. Here correlation is somewhat stronger. Why should confidence be an indicator of higher citations? A plausible explanation is that there is confounder driving both variables. For example, it might be that papers which are easier to understand (due to elegance of the idea, or quality of exposition) inspire greater reviewer confidence and also increase the number of citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d2fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'average_confidence'\n",
    "filter_col = \"accept\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ma.write_figure(filename=\"citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329631f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
